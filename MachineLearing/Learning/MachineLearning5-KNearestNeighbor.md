# 机器学习——K近邻（k-nearest neighbor）

<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

## k近邻算法

给定一个训练数据集，对新的输入实例，在训练数据中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。

**最近邻算法**：在k近邻算法中当k=1时，称为最近邻算法。

## k近邻模型

k近邻算法使用的模型实际上对应于对特征空间的划分，该模型由三个基本要素：距离度量、k值的选择和分类决策规则。

### 距离度量

k近邻模型的特征空间一般是n维实数向量空间，使用欧氏距离，但也可以是其他距离，如<img src="http://www.forkosh.com/mathtex.cgi? L_{p}">距离、Minkowski距离。不同距离确定的最近邻点是不同的。<img src="http://www.forkosh.com/mathtex.cgi? L_{p}">距离定义为：

$$
L\_{p}(x\_{i},x\_{j})=(\sum^{n}\_{l=1}|x^{(l)}\_{i}-x^{(l)}_{j}|^{p})^{\frac{1}{p}}
$$

### k值的选择

在应用中，k值一般取一个比较小的数值。通常采用的交叉验证来选择最优的k值。

### 分类决策规则

k近邻法中的分类决策规则往往是多数表决，多数表决规则等价于经验风险最小化。

## k近邻法的实现：kd树

k近邻法最简单的实现方法是线性扫描，这种方法需要计算输入实例与每个训练实例的距离。当训练集很大时，计算非常耗时。为了提高k近邻的搜索效率，可以考虑使用特殊的结构存储训练数据，来减少计算距离的次数。kd树（kd tree）就是一种方法。

### 构造kd树

kd树是二叉树，表示对k维空间的一个划分。构造kd树相当于不断的用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域，kd树的每个节点对应于一个k维超矩形区域。

步骤

