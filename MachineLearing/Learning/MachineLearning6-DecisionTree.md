# 机器学习——决策树（Decision Tree）

<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

决策树的学习通常包括3步：特征选择、决策树的生成和决策树的修剪。经典的算法有ID3、C4.5和CART算法。

## 决策树模型

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由**结点（node）**和**有向边（directed edge）**组成。结点有两种类型：**内部结点**和**叶结点**。内部结点表示一个特征或属性，叶结点表示一个类。

决策树学习的策略是以损失函数为目标函数的最小化。损失函数通常是正则化的极大似然函数。损失函数确定后，学习问题变为在损失函数意义下选择最优决策树的问题。

## 特征选择

特征选择在于选取对训练数据有分类能力的特征。常用的特征选择准则是信息增益或信息增益比。

**信息增益：**是特征选择的一个重要指标，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，改特征越重要。（信息增益=熵-条件熵）
**信息增益率：**

## ID3算法（Iterative Dichotomiser 3，迭代二叉树3代）

ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归的构建决策树。具体方法：

1. 从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点。
2. 对子结点递归的调用以上方法，构建决策树。直到所有特征的信息增益均很小或没有特征可以选择为止。

ID3算法思想描述：

1. 对当前样本集合，计算属性的信息增益
2. 选择信息增益最大的特征a作为根节点
3. 把在特征a处取值相同的样本归为同一个子集，a取几个值就得到几个子集
4. 依次对每种情况下的子集，递归调用建树
5. 若子集只含有单个特征，则分支为叶子节点，判断其特征并标上相应的符号，然后返回调用处。

## C4.5算法

C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进，C4.5在生成的过程中，用信息增益率来选择特征。

## 决策树的剪枝

决策树剪枝是从已经生成的树上裁剪掉一下子树或叶结点，并将根节点或父结点作为新的叶结点。

## CART算法

分类与回归树（classification and regression tree, CART）是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右边分支是取值为“否”的分支。

### GINI（基尼）指数
特征包含的类别越复杂，GINI指数就越大。

**基尼指数：**是另一种数据的不纯度的度量方法。
$$
Gini(D)=1-\sum^{m}_{i=1}p^{2}\_{i}
$$

其中m表示数据集中类别的个数，p_{i}表示数据集中任意特征前提下属于第i类的概率。

**基尼增益：**类似于信息增益。最好的划分为基尼增益最小时的划分。

### CART剪枝

首先从生成算法产生的决策树底端开始不断剪枝，直到根结点，形成一个子树序列；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。